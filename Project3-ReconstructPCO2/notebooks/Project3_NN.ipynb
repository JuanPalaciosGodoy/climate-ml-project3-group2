{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80f2b2c-3e4c-4ce3-a977-c822989b3e67",
   "metadata": {},
   "source": [
    "# **Machine Learning Reconstruction of Surface Ocean pCO₂**\n",
    "Spring 2025, EESC4243/STAT4243/5243 “Climate Prediction Challenges with Machine Learning”, Columbia University\n",
    "\n",
    "## Introduction \n",
    "The ocean plays a crucial role in the global carbon cycle by absorbing atmospheric CO₂, having absorbed 38% of all anthropogenic fossil fuel emissions over the industrial era. \n",
    "\n",
    "Accurate estimation of air–sea CO₂ flux is critical for understanding the current and future global carbon budget, yet remains challenging due to the sparse and unevenly distributed nature of surface ocean pCO₂ observations. The **Surface Ocean CO₂ Atlas (SOCAT)** database (https://socat.info/) provides the most extensive dataset available, but its coverage is limited to only about 2% of all boxes of 1 degree X 1 degree (= 100km X 100km at the equator) over the last several decades. Data are particularly limited in high-latitude regions and during winter months.\n",
    "\n",
    "To fill in the gaps in these data, statistical and machine learning (ML) techniques have been widely used to reconstruct global pCO₂ fields by interpolating between observations using environmental predictors such as sea surface temperature (SST), sea surface salinity (SSS), mixed layer depth (MLD), chlorophyll-a (Chl-a), and atmospheric CO₂ (xCO₂).\n",
    "\n",
    "How good are these methods? Since the real full-coverage pCO2 of the ocean is unknown, we need another approach to assess the skill of ML-based reconstructions. Our answer is the **Large Ensemble Testbed (LET)**, which provides full-coverage pCO₂ output from Earth System Models, as well as associated driver varibles also from the ESM. In the context of this testbed, we can sample the pCO2 in the same pattern as real-world SOCAT and then reconstruct. Since the full-field pCO2 field is known (\"Model Truth\"), we can directly evaluate of ML reconstruction performance. With this approach, it has been shown that ML methods can capture seasonal variability well, they often overestimate decadal variability, particularly in regions with limited data coverage (Gloege et al. 2021).\n",
    "\n",
    "This study builds upon previous work by incorporating a **pCO₂-Residual** approach to improve ML-based pCO₂ reconstructions. The **pCO₂-Residual** method removes the temperature-driven component from pCO₂ before applying ML, thereby reducing the dominance of temperature in predictions and enhancing the ability of the model to capture non-temperature-driven variability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5841700-a1f2-4ca9-9708-6e01b5539535",
   "metadata": {},
   "source": [
    "\n",
    "We reproduce a portion of the analysis carried out by  \n",
    "**Gloege et al. (2021)** *\"Quantifying Errors in Observationally Based Estimates of Ocean Carbon Sink Variability.\"* **Global Biogeochemical Cycles** 34: e2020GB006788.  \n",
    "([DOI: 10.1029/2020GB006788](https://doi.org/10.1029/2020GB006788))  \n",
    "\n",
    "using the method of  \n",
    "**Bennington et al. (2022)** *\"Explicit Physical Knowledge in Machine Learning for Ocean Carbon Flux Reconstruction: The pCO2-Residual Method\"*. **Journal of Advances in Modeling Earth Systems**, 14(10). ([DOI: 10.1029/2021ms002960](https://doi.org/10.1029/2021ms002960))\n",
    "\n",
    "Our study:\n",
    "1. Implements an **XGBoost-based pCO₂-Residual reconstruction**  (Bennington et al. 2022).\n",
    "2. Implements a **Feed Forward Neural Network**\n",
    "3. Evaluates reconstruction performance using a Large Ensemble Testbed, with **bias and correlation metrics** as in Gloege et al (2021).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163e5b0-d999-4dfb-ab8c-40f8e3318b5e",
   "metadata": {},
   "source": [
    "### Running Notes\n",
    "1. Users need to enter their GitHub/LEAP-Pangeo username at the end of Section 2.\n",
    "2. Several time-consuming steps have outputs saved to files, so they only need to be run initially or if changes are made:  \n",
    "    - Section 4.3: ML Training (~1 minute/ESM member)  \n",
    "    - Section 4.4: Reconstruction/Inference (~1 minute/ESM member)  \n",
    "    - Section 4.4.1: Summation of pCO2-Residual and pCO2-T to recover pCO2 (~15 seconds/ESM member)\n",
    "\n",
    "\n",
    "    With a **128GB CPU**, actual runtimes may vary based on system load and selected members, but this serves as a general guideline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51674379-0ce7-41b3-b6ce-f654fc6f89a1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>your_username:</b> The username of the person running the code. \n",
    "<p><b>owner_username:</b> The username of the notebook owner.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892580c-f016-4fc2-94f9-60912cebcc6e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>runthiscell:</b>(Default = 1) Disable a cell by setting <b>runthiscell=0</b>. Reviewers should set <b>runthiscell=-1</b> to save time and space. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1bb72f-9997-4e54-a0d6-f0e28f87d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_username = 'JuanPalaciosGodoy'  # username of the person running the code. Reviewers should also change this to their own username.\n",
    "\n",
    "#To allow the reviewer to access the saved files, provide notebook owner's username here:  \n",
    "owner_username = 'JuanPalaciosGodoy'  # Reviewer should not change this name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c6783-624f-486a-bafe-672ec7a9d73c",
   "metadata": {},
   "source": [
    "## **2.1 Input (Features) Sources**\n",
    "The **input features** for the machine learning model are derived from **pre-processed Earth System Model (ESM) outputs or inputs**\n",
    "\n",
    "### **Feature and Target Variables for the ML Model**\n",
    "The features used for training the **pCO₂-Residual machine learning model** are listed below:\n",
    "\n",
    "\n",
    "| **Feature** | **Description** | **ESM Input or Output?** | \n",
    "|------------|----------------|----------------|\n",
    "| **SST (Sea Surface Temperature)** | Simulated ocean surface temperature | **ESM Output** |  \n",
    "| **SST_anom** | SST anomaly from climatology | **ESM Output** |  \n",
    "| **SSS (Sea Surface Salinity)** | Simulated surface ocean salinity | **ESM Output** |  \n",
    "| **SSS_anom** | SSS anomaly from climatology | **ESM Output** |  \n",
    "| **MLD_clim_log** | Log-transformed climatological mixed layer depth | **ESM Output** |  \n",
    "| **Chl-a (Chlorophyll concentration)** | Proxy for biological activity (log-transformed) | **ESM Output** |  \n",
    "| **Chl-a_anom** | Anomaly of Chl-a | **ESM Output** |  \n",
    "| **xCO₂ (Atmospheric CO₂ concentration)** | Atmospheric CO₂ mole fraction | **ESM Input, from data** |  \n",
    "| **A, B, C** | Space on the globe | **See Bennington et al. 2022, Table 1** |  \n",
    "| **T0, T1** | Time | **See Bennington et al. 2022, Table 1** |  \n",
    "\n",
    "\n",
    "The **target variable** for reconstruction is:\n",
    "- **pCO₂-Residual**: This deviation from the temperature-driven component of pCO₂, reducing SST's dominance in ML reconstructions and improving model performance in data-sparse regions (detailed below).\n",
    "- **pCO₂-Residual-trend**: This is the trend component of pCO₂-Residual\n",
    "- **pCO₂-Residual-seasonal**: This is the seasonal component of pCO₂-Residual\n",
    "- **pCO₂-Residual-deseasonal**: This is the de-seasonal component of pCO₂-Residual\n",
    "\n",
    "### **Key Considerations:**\n",
    "- **pCO₂-Residual**: By removing the temperature-driven component from pCO₂, we enhance the ability of machine learning models to capture **non-temperature-driven variability**, particularly in poorly observed regions.\n",
    "- **pCO₂-Residual** = pCO₂-Residual-trend + pCO₂-Residual-seasonal + pCO₂-Residual-deseasonal\n",
    "- **Data Subsampling Based on SOCAT**: The use of a **SOCAT-derived mask** ensures that the ML model is trained and evaluated using a realistic observational distribution, mitigating potential biases from uneven data coverage.\n",
    "\n",
    "### **Final Input Structure**:\n",
    "- **Feature Matrix**: `(N, 12)`, where `N` represents the number of samples, and 12 predictor variables are used.\n",
    "- **Target Variable**: `pCO₂-Residual`, which the model aims to reconstruct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41349631-8b5a-4750-ac9e-e65431db3600",
   "metadata": {},
   "source": [
    "# 0.1. Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4259b69-596f-403b-80ef-a4db5dad096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for ML:\n",
    "features_sel = ['sst','sst_anom','sss','sss_anom','mld_clim_log','chl_log','chl_log_anom','xco2','A', 'B', 'C', 'T0', 'T1']\n",
    "\n",
    "# the target variable we reconstruct:\n",
    "target_sel = ['pco2_residual_deseasonal'] #pco2_residual: this represents pCO2 - pCO2-T (calculated in notebook 00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78934e6-b3db-4412-a8fc-9f85b6daee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range\n",
    "__MODEL_TYPE__ = \"nn\" # \"xgb\" or \"nn\"\n",
    "__GRID_SEARCH_APPROACH__ = 'nmse'\n",
    "__DATE_RANGE_START__ = '2004-01-01T00:00:00.000000000'\n",
    "__DATE_RANGE_END__ = '2023-12-31T00:00:00.000000000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80186cf7-27c4-4f0c-8c56-44c90c649b76",
   "metadata": {},
   "source": [
    "# 0. Setup Workspace and Import Packages\n",
    "We use %%capture to suppress output and keep the notebook clean. However, feel free to remove it if you want to check warnings or logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37add44-d2ae-4e76-bc1a-effea1531dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42d82f8-8400-459b-890c-f8ebfc9799e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745471158.205493    4188 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745471158.212611    4188 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745471158.230590    4188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745471158.230611    4188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745471158.230613    4188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745471158.230616    4188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "### standard imports ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import fsspec\n",
    "import torch\n",
    "### Python file with supporting functions ###\n",
    "# standard imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import sys\n",
    "# Redirect all low-level stderr output\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "import csv\n",
    "import cmocean as cm\n",
    "\n",
    "# machine learning libraries\n",
    "import xgboost as xgb     \n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Change to the parent directory of the current working directory. (Run only once—otherwise it will keep moving up the directory tree)\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Python file with supporting functions\n",
    "import lib.residual_utils as supporting_functions\n",
    "from lib.visualization import *\n",
    "from lib.bias_figure2 import concat_datasets, XarrayEvaluator\n",
    "from lib.corr_figure3 import eval_spatial\n",
    "from lib.paths_utils import SavingPaths\n",
    "from lib.model_utils import train_member_models, Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9df76f-dfcb-4044-9a7e-c096f121e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting the date range to unify the date type ###\n",
    "\n",
    "# create date vector, adds 14 days to start & end\n",
    "dates = pd.date_range(start=__DATE_RANGE_START__, \n",
    "                      end=__DATE_RANGE_END__,freq='MS')\n",
    "\n",
    "\n",
    "init_date = str(dates[0].year) + format(dates[0].month,'02d')\n",
    "fin_date = str(dates[-1].year) + format(dates[-1].month,'02d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb0d50-93db-49cf-9f2c-451d007df70e",
   "metadata": {},
   "source": [
    "\n",
    "### **Output Data Storage and Model Saving**\n",
    "\n",
    "The output data generated by this notebook, including model predictions, performance metrics, and trained models, is saved in a cloud-based environment using a user-specific directory structure. This ensures that each user’s results are organized and accessible without interfering with others’ work.\n",
    "\n",
    "The output data is organized into three main categories:  \n",
    "1. **Machine Learning Models:** Trained models are saved for future use, ensuring that results can be replicated without re-training.  \n",
    "2. **Reconstructions:** Predicted pCO₂ fields are stored for further analysis and visualization.  \n",
    "3. **Performance Metrics:** CSV files containing test and unseen data performance metrics are saved for easy evaluation.\n",
    "\n",
    "### **Data Sources and Paths**\n",
    "The data is stored in a **cloud environment, LEAP-Pangeo**, ensuring efficient access and scalability for the machine learning workflow. Key datasets include:\n",
    "\n",
    "- **Ensemble dir**:\n",
    "\n",
    "  Contains the original data from pre-processed Earth System Model (ESM) outputs, available for 100+ ESM members. For computational efficiency, we rely on a selection of this dataset compiled by TA Xinyi Ke. The full ensemble data is available and could be explored, with due consideration of storage constraints. \n",
    "  \n",
    "- **ML Input and Target Data**:\n",
    "\n",
    "    Provides a dataframe-format dataset containing preprocessed ML inputs and ground truth from a selected subset of ensemble members. You may also generate your own dataset for a custom selection or range of members (see reference: Project3_Data.ipynb). Due to limited GCS storage, we recommend using the provided dataset for most projects.\n",
    "  \n",
    "- **SOCAT Data (Mask File)**:  \n",
    "\n",
    "  Masking file based on real-world **SOCAT pCO₂ observations**. Here, these data are not used directly, but are input solely so that their sampling pattern in space and time can be applied to model pCO2 fields, thus mimicing real-world observational density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fe335d-44e6-4d74-9697-50fc5e5780c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loading and saving paths\n",
    "saving_paths = SavingPaths(\n",
    "    your_username=your_username,\n",
    "    owner_username=owner_username,\n",
    "    init_date=init_date,\n",
    "    fin_date=fin_date,\n",
    "    grid_search_approach=__GRID_SEARCH_APPROACH__,\n",
    "    model=__MODEL_TYPE__\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb335b-cbbb-4c58-9482-466ee7735f2a",
   "metadata": {},
   "source": [
    "# 1. Surface ocean pCO2: A sparse data challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0931f-c23f-4794-9862-c001ba1d32b5",
   "metadata": {},
   "source": [
    "SOCAT data coverage is uneven, with some regions, particularly in the Southern Hemisphere and open ocean areas, having significantly fewer observations. Regions with denser observational coverage, such as the Northern Hemisphere, tend to have lower biases in CO2 flux reconstructions compared to sparsely sampled areas like the Southern Ocean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f69c8-22f6-4d6c-9ab6-5748f9d4df36",
   "metadata": {},
   "source": [
    "# 2. Data Introduction and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febda34-4029-426c-aa0d-34445a3635f8",
   "metadata": {},
   "source": [
    " **Compute the temperature-driven component** of pCO₂:\n",
    "\n",
    "$$\n",
    "\\text{pCO}_{2,T} = \\overline{\\text{pCO}_2} \\cdot \\exp\\left[ 0.0423 \\cdot (T - \\overline{T}) \\right]\n",
    "$$\n",
    "\n",
    "- $ \\overline{\\text{pCO}_2} $: long-term mean of surface ocean pCO₂  \n",
    "- $ T $: sea surface temperature at a given time/location  \n",
    "- $ \\overline{T} $: long-term mean sea surface temperature  \n",
    "- $ 0.0423 $: empirically derived constant (from Takahashi et al., 1993)\n",
    "\n",
    "\n",
    " **Subtract to get the residual**:\n",
    "\n",
    "$$\n",
    "\\text{pCO}_{2,\\text{Residual}} = \\text{pCO}_2 - \\text{pCO}_{2,T}\n",
    "$$\n",
    "\n",
    "The residual is then decomposed into three separate components such that:\n",
    "\n",
    "$$\n",
    "\\text{pCO}_{2,\\text{Residual}} = \\text{pCO}_{2,seasonal} + \\text{pCO}_{2,deseasonal} + \\text{pCO}_{2,trend}\n",
    "$$\n",
    "\n",
    "The seasonal and deseasonal components are then used as the **target variables** in ML training, helping disentangle the direct solubility-driven temperature effect from other biogeochemical processes.\n",
    "\n",
    "In our workflow, this calculation was done during processing of the Earth System Model (ESM) dataset, not included in this notebook. The datasets under `ensemble_dir` include the variable `pCO2_T`. For this notebook, we will directly use the preprocessed ML input dataset, which includes `pco2_residual`.\n",
    "\n",
    "Later in the notebook, we recover the total pCO2 by **adding the temperature component back** to the residual:\n",
    "\n",
    "$$\n",
    "\\text{pCO}_2 = \\text{pCO}_{2,\\text{Residual}} + \\text{pCO}_{2,T}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a655f4-fa0a-4a42-ab1c-acbbb3d1de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up for getting files from leap bucket ###\n",
    "fs = gcsfs.GCSFileSystem()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1022e3-160e-49e7-8620-83add5264a18",
   "metadata": {},
   "source": [
    "# 3.  Earth System Models and their Ensemble Members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec31f06e-b223-46a8-8964-2b55d03f0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "mems_dict = dict()\n",
    "\n",
    "# Get all paths\n",
    "all_paths = fs.ls(saving_paths.inputs_path)\n",
    "\n",
    "for ens_path in all_paths:             \n",
    "    ens = ens_path.split('/')[-1]\n",
    "    mems = fs.ls(ens_path)\n",
    "    for mem in mems:        \n",
    "        memo = mem.split('/')[-1]\n",
    "        if ens not in mems_dict:\n",
    "            mems_dict[ens] = [memo]\n",
    "        elif ens in mems_dict:\n",
    "            mems_dict[ens].append(memo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e36a5e-a4b9-4bce-83df-8b825d653d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACCESS-ESM1-5': ['member_r10i1p1f1', 'member_r5i1p1f1', 'member_r2i1p1f1'], 'CanESM5': ['member_r2i1p1f1', 'member_r1i1p2f1', 'member_r1i1p1f1'], 'MPI-ESM1-2-LR': ['member_r12i1p1f1', 'member_r11i1p1f1', 'member_r15i1p1f1']}\n"
     ]
    }
   ],
   "source": [
    "## Here you can change which models and how many members you use\n",
    "random.seed(42)  # Set seed for reproducibility\n",
    "\n",
    "selected_ensembles = ['ACCESS-ESM1-5', 'CanESM5', 'MPI-ESM1-2-LR']\n",
    "\n",
    "selected_members_dict = {esm: mems_dict[esm] for esm in selected_ensembles}\n",
    "\n",
    "selected_mems_dict = {}\n",
    "num_members = 3  # Set the number of ensemble members from each ESM\n",
    "\n",
    "for ens, members in selected_members_dict.items():\n",
    "    if len(members) >= num_members:\n",
    "        selected_mems_dict[ens] = random.sample(members, num_members)  # Select `num_members` random members\n",
    "    else:\n",
    "        selected_mems_dict[ens] = members  # If there are fewer members than `num_members`, select all\n",
    "\n",
    "print(selected_mems_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e1073-7eca-425e-877b-3d6e0e017fba",
   "metadata": {},
   "source": [
    "# 4. ML Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df916e52-50fa-44ff-a767-0d046bdb8a77",
   "metadata": {},
   "source": [
    "## 4.1 Data Split\n",
    "\n",
    "We split data to training data set and testing dataset based on date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9ac751-d5fd-44b3-b137-1402e11ce5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train-validate-test split proportions ###\n",
    "\n",
    "select_dates = []\n",
    "test_dates = []\n",
    "\n",
    "for i in range(0,len(dates)):\n",
    "    if i % 5 != 0:\n",
    "        select_dates.append(dates[i]) ### 80% train days set ###\n",
    "    if i % 5 == 0:\n",
    "        test_dates.append(dates[i]) ### 20% test days set ### \n",
    "\n",
    "### Then, the month numbers above are converted back to their respective datetime objects.\n",
    "\n",
    "year_mon = []\n",
    "\n",
    "for i in range(0,len(select_dates)):\n",
    "    \n",
    "    tmp = select_dates[i]\n",
    "    year_mon.append(f\"{tmp.year}-{tmp.month}\")\n",
    "    \n",
    "test_year_mon = []\n",
    "\n",
    "for i in range(0,len(test_dates)):\n",
    "    \n",
    "    tmp = test_dates[i]\n",
    "    test_year_mon.append(f\"{tmp.year}-{tmp.month}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c92ad9-b5c1-478c-95e3-1e1383de39b5",
   "metadata": {},
   "source": [
    "## 4.2 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d1c3160-bd76-46fc-a099-dc27add89ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "__SEED__ = 10\n",
    "torch.manual_seed(__SEED__)\n",
    "\n",
    "if Models(__MODEL_TYPE__) == Models.NEURAL_NETWORK:\n",
    "    params = {\n",
    "        'input_nodes':13,\n",
    "        'hidden_nodes':50,\n",
    "        'output_nodes':1,\n",
    "        'epochs':3000,\n",
    "        'lr': 1e-03\n",
    "    }\n",
    "elif Models(__MODEL_TYPE__) == Models.XGBOOST:\n",
    "    params = {\n",
    "        'n_estimators': 500,  # Number of boosting rounds\n",
    "        'max_depth': 6,  # Maximum depth of each tree to control model complexity\n",
    "        'learning_rate': 0.05,  # Step size shrinkage to prevent overfitting\n",
    "        'subsample': 0.8,  # Fraction of samples used for training each tree\n",
    "        'colsample_bytree': 0.8,  # Fraction of features used per tree\n",
    "        'gamma': 0.1,  # Minimum loss reduction required for further partitioning\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight in a leaf node\n",
    "        'reg_alpha': 0.1,  # L1 regularization to reduce model complexity\n",
    "        'reg_lambda': 1.0,  # L2 regularization for preventing overfitting\n",
    "        'objective': 'reg:squarederror',  # Loss function for regression tasks\n",
    "        'n_jobs': 30,  # Number of parallel threads to use for training\n",
    "        'eval_metric': 'rmse',\n",
    "        'early_stopping_rounds': 50  # Stop training if performance doesn't improve for 50 rounds\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(f\"model {__MODEL_TYPE__} not supported! Please choose a model in [`{Models.NEURAL_NETWORK.value}`, `{Models.XGBOOST.value}`]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d0e20-666e-46b0-9ae6-fa7b3e40f5e3",
   "metadata": {},
   "source": [
    "### Important Note: Run ML (4.3) and Reconstruction (4.4) Part Only Once\n",
    "\n",
    "The **ML Reconstruction** step needs to be run **only once** for each ML model developed. This helps save computational resources and execution time.\n",
    "\n",
    "The reconstruction data is saved under our **own username-specific workspace** in GCS. This means that even if you exit and re-enter JupyterHub, your data will remain available, eliminating the need for reprocessing.\n",
    "\n",
    "### Before Running Again:\n",
    "Before re-running the ML training steps, make sure a new experiment is actually necessary. Avoiding redundant computations helps optimize time and resource usage. It's also a good idea to monitor your storage regularly and clean up unnecessary files. If you're certain that no new experiment is needed, you can comment out the relevant code (set runthiscell = \"0\") to prevent accidental re-execution.\n",
    "\n",
    "For reviewer, set runthiscell to -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f267d8-fe61-41f4-a7e4-593f1f18d478",
   "metadata": {},
   "source": [
    "## ML train/val/test data\n",
    "The ML model was trained on **masked synthetic data** that mimics real-world observational sampling patterns (SOCAT). Specifically, for each ensemble member, we:\n",
    "\n",
    "1. **Filtered valid samples** by selecting grid cells that have:\n",
    "   - No missing values in input features or the target (`pCO₂-Residual`),\n",
    "   - Physically realistic `pCO₂-Residual` values (between -250 and 250 μatm),\n",
    "   - An ocean mask indicating valid ocean regions.\n",
    "\n",
    "2. **Identified SOCAT-like samples** using a binary `socat_mask`.  \n",
    "   - We defined the **training pool** as grid cells where **`socat_mask == 1`**, and the time falls within a list of pre-selected training months (`year_mon`).\n",
    "   - Similarly, **testing data** was drawn from SOCAT-like samples falling into the `test_year_mon` time range.\n",
    "\n",
    "3. **Performed a secondary train/val split** (within the training pool) using a stratified random seed matrix (`random_seeds`), where the seed location is tied to each ensemble member to ensure reproducibility and model diversity across members.\n",
    "\n",
    "4. The **“unseen” data**, i.e., where **`socat_mask == 0`** but data is otherwise valid, was reserved for reconstruction evaluation in non-observed regions.\n",
    "\n",
    "This ensures that:\n",
    "- Training and testing sets do **not overlap in time** (`year_mon` vs. `test_year_mon`),\n",
    "- And are drawn from the same spatial sampling mask, preserving the real-world SOCAT sampling pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56c2a9-7e31-4135-927a-df32ff612c1e",
   "metadata": {},
   "source": [
    "## 4.3 ML Training\n",
    "\n",
    "To avoid re-run the ML training, set runthiscell = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b065b-ed58-471f-8ed2-bc3311325555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 05:06:02.355727\n",
      "ACCESS-ESM1-5 member_r10i1p1f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 55/3000 [02:35<2:19:05,  2.83s/epoch, patience_count=20, train_loss=8, valid_loss=7.39]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 55.\n",
      "Starting local model saving process...\n",
      "Model successfully saved locally at: output/model_saved/model_pCO2_2D_ACCESS-ESM1-5_r10i1p1f1_mon_1x1_200401_202312.pth\n",
      "Local model saving process complete.\n",
      "test performance metrics: {'mse': 125.29973621051266, 'mae': 7.2110815909533175, 'medae': 4.673835754394531, 'max_error': 177.9755401611328, 'bias': 0.1344061642885208, 'r2': -8913.246131408314, 'corr': 0.12848341464996338, 'cent_rmse': 11.193736472264865, 'stdev': 0.1185302734375, 'amp_ratio': 0.0019671483896672726, 'stdev_ref': 11.207666397094727, 'range_ref': 349.9870910644531, 'iqr_ref': 9.363887786865234}\n",
      "ACCESS-ESM1-5 member_r5i1p1f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▎         | 112/3000 [04:53<2:06:15,  2.62s/epoch, patience_count=20, train_loss=7.64, valid_loss=7.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 112.\n",
      "Starting local model saving process...\n",
      "Model successfully saved locally at: output/model_saved/model_pCO2_2D_ACCESS-ESM1-5_r5i1p1f1_mon_1x1_200401_202312.pth\n",
      "Local model saving process complete.\n",
      "test performance metrics: {'mse': 108.72159398041039, 'mae': 6.841115483578023, 'medae': 4.531004905700684, 'max_error': 122.90266418457031, 'bias': 0.04093089699745178, 'r2': -12958.45237601523, 'corr': 0.11495447158813477, 'cent_rmse': 10.426964753964137, 'stdev': 0.091552734375, 'amp_ratio': 0.0024249630514532328, 'stdev_ref': 10.437129020690918, 'range_ref': 229.34466552734375, 'iqr_ref': 9.092084884643555}\n",
      "ACCESS-ESM1-5 member_r2i1p1f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  56%|█████▌    | 1675/3000 [1:12:46<57:33,  2.61s/epoch, patience_count=20, train_loss=5.61, valid_loss=5.32]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 1675.\n",
      "Starting local model saving process...\n",
      "Model successfully saved locally at: output/model_saved/model_pCO2_2D_ACCESS-ESM1-5_r2i1p1f1_mon_1x1_200401_202312.pth\n",
      "Local model saving process complete.\n",
      "test performance metrics: {'mse': 72.65336007265022, 'mae': 5.507317725788666, 'medae': 3.6144349575042725, 'max_error': 134.89132690429688, 'bias': -0.0173739492893219, 'r2': 1.0, 'corr': 0.0, 'cent_rmse': 8.523694039127063, 'stdev': 8.4453125, 'amp_ratio': 0.5332518219947815, 'stdev_ref': 12.677984237670898, 'range_ref': 299.3426208496094, 'iqr_ref': 9.71685791015625}\n",
      "CanESM5 member_r2i1p1f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 212/3000 [09:07<1:58:25,  2.55s/epoch, patience_count=0, train_loss=6.4, valid_loss=6.31] "
     ]
    }
   ],
   "source": [
    "runthiscell = 1  # 0 will turn off, 1 will turn on, -1 will only run the first member. Reviewers should set this to -1.\n",
    "\n",
    "seed_loc_dict = defaultdict(dict)\n",
    "for ens,mem_list in mems_dict.items():\n",
    "    sub_dictt = dict()\n",
    "    for no,mem in enumerate(mem_list):\n",
    "        sub_dictt.update({mem:no})\n",
    "    seed_loc_dict.update({ens:sub_dictt})\n",
    "\n",
    "if runthiscell:  \n",
    "\n",
    "    if runthiscell == -1:\n",
    "        print(\"Reviewing process: Running ML only for the first member of the first ESM.\")\n",
    "        first_ens = list(selected_mems_dict.keys())[0]  # Get the first ensemble key\n",
    "        first_mem = selected_mems_dict[first_ens][0]   # Get the first member in that ensemble\n",
    "        run_selected_mems_dict = {first_ens: [first_mem]}  # Create a dictionary with only the first ensemble and member\n",
    "    else:\n",
    "        run_selected_mems_dict = selected_mems_dict\n",
    "    \n",
    "    train_member_models(\n",
    "        saving_paths=saving_paths,\n",
    "        features=features_sel,\n",
    "        target=target_sel,\n",
    "        train_year_mon=year_mon,\n",
    "        test_year_mon=test_year_mon,\n",
    "        run_selected_mems_dict=run_selected_mems_dict,\n",
    "        seed_loc_dict=seed_loc_dict,\n",
    "        dates=dates,\n",
    "        model_type=__MODEL_TYPE__,\n",
    "        is_training=True,\n",
    "        **params\n",
    "    )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2fe26b-5ed1-4f35-b225-5f084bddbead",
   "metadata": {},
   "source": [
    "## 4.4 Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd31b2-96e8-47aa-a5de-bbbd01c02411",
   "metadata": {},
   "source": [
    "### What Are We Reconstructing?\n",
    "\n",
    "After training the model, we generate pCO₂ predictions not just for evaluation but also for reconstructing spatial fields across different sample categories:\n",
    "\n",
    "1. **`unseen_sel`**: These are grid points that are valid (no missing values, within physical bounds) but **not observed** in the SOCAT dataset (i.e., `socat_mask == 0`). Predictions on these samples (`y_pred_unseen`) test the model’s ability to generalize beyond observed regions.\n",
    "\n",
    "2. **`sel`**: These are SOCAT-like samples where `socat_mask == 1`. They include both training and test data (depending on the year/month). Predictions here (`y_pred_seen`) are used to assess performance where observations exist.\n",
    "\n",
    "\n",
    "### Explanation of Reconstruction Output Variables\n",
    "\n",
    "Each column added to the DataFrame (`df`) serves a specific purpose in evaluation and reconstruction:\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `pCO2_truth` | The actual pCO₂ residual values|\n",
    "| `pCO2_recon_full` | The full reconstruction result (predicted values across both SOCAT and unseen regions). |\n",
    "| `pCO2_recon_unseen` | Predicted values only for unseen regions (where `socat_mask == 0`). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732161a2-6b0d-4266-b79d-ff0eeaee9324",
   "metadata": {},
   "source": [
    "If you have not changed your ML, the reconstruction step (~1 minute/member) does not need to be re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38990cc7-f40c-44a2-a70c-5e5615d74b3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runthiscell = 1  # 0 will turn off, 1 will turn on, -1 will only run the first member. Reviewers should set this to -1.\n",
    "\n",
    "if runthiscell:\n",
    "\n",
    "    if runthiscell == -1:\n",
    "        print(\"Reviewing process: Running reconstrunction only for the first member of the first ESM.\")\n",
    "        first_ens = list(selected_mems_dict.keys())[0]  # Get the first ensemble key\n",
    "        first_mem = selected_mems_dict[first_ens][0]   # Get the first member in that ensemble\n",
    "        run_selected_mems_dict = {first_ens: [first_mem]}  # Create a dictionary with only the first ensemble and member\n",
    "    else:\n",
    "        run_selected_mems_dict = selected_mems_dict\n",
    "\n",
    "    train_member_models(\n",
    "        saving_paths=saving_paths,\n",
    "        features=features_sel,\n",
    "        target=target_sel,\n",
    "        train_year_mon=year_mon,\n",
    "        test_year_mon=test_year_mon,\n",
    "        run_selected_mems_dict=run_selected_mems_dict,\n",
    "        seed_loc_dict=seed_loc_dict,\n",
    "        dates=dates,\n",
    "        model_type=__MODEL_TYPE__,\n",
    "        is_training=False,\n",
    "        **params\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e46b5-96a1-4838-841a-cced36a81ee7",
   "metadata": {},
   "source": [
    "### 4.4.1 Add pCO2-seasonal + pCO2-deseasonal + pCO2-Temperature back to reconstructed pCO2-Residual, thus recovering pCO2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4348efb7-7609-4968-87b7-ca7037620fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runthiscell = 1  # 0 will turn off, 1 will turn on\n",
    "\n",
    "if runthiscell:\n",
    "    if runthiscell == -1:\n",
    "        supporting_functions.calc_recon_pco2_modified(saving_paths.ensemble_dir, saving_paths.recon_output_dir, saving_paths.recon_output_dir_alternate, selected_mems_dict, init_date, fin_date, owner_username)\n",
    "    else:\n",
    "        supporting_functions.calc_recon_pco2_modified(saving_paths.ensemble_dir, saving_paths.recon_output_dir, saving_paths.recon_output_dir_alternate, selected_mems_dict, init_date, fin_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b62ad4-4442-409f-b818-b42be3a3dbb4",
   "metadata": {},
   "source": [
    "**For the owner who completed the experiment and reconstructed the data:**\n",
    "\n",
    "1. Ensure that the reconstruction data you saved under the specified path is clean, accurate, and ready for sharing.\n",
    "2. Only the data you wish to save and provide to reviewers should be kept.\n",
    "3. Change the permissions to allow others to read the data, ensuring it's accessible to reviewers.\n",
    "\n",
    "\n",
    "**Reviewers should not run this cell**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74cb4dd-d141-45b0-a782-780e7eb376a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "if runthiscell != -1:\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(\"leap-persistent\")\n",
    "    \n",
    "    prefix = f\"{owner_username}/{owner_username}/pco2_residual/nmse/post02_xgb/reconstructions/\"\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    \n",
    "    seen_dirs = set()  # Track top-level directories (immediate subdirectories of `reconstructions`)\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Extract the relative path after `reconstructions/`\n",
    "        relative_path = blob.name[len(prefix):]\n",
    "        top_level_dir = relative_path.split(\"/\")[0]  # Get first component\n",
    "\n",
    "        try:\n",
    "            # Make the file public\n",
    "            blob.make_public()\n",
    "            \n",
    "            # Only print if it's a new top-level directory\n",
    "            if top_level_dir not in seen_dirs:\n",
    "                seen_dirs.add(top_level_dir)\n",
    "                print(f\"Made public: {top_level_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to make public: {blob.name}\")\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6674a86-9b05-4234-9475-c8ad54a8539e",
   "metadata": {},
   "source": [
    "###  4.4.3  Visualize the reconstruction for 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602feb8-a0e5-4b22-bd9f-221451d871b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plot_style = \"seaborn-v0_8-talk\"\n",
    "cmap = cm.cm.thermal\n",
    "cbar_title = 'pCO₂ (µatm)'\n",
    "vrange = [280, 440]  # Colorbar range\n",
    "\n",
    "# Select the first ensemble and member from the dictionary\n",
    "first_ens = list(selected_mems_dict.keys())[0]\n",
    "first_mem = selected_mems_dict[first_ens][0]\n",
    "\n",
    "# Load original member data from ESM output\n",
    "member_dir = f\"{saving_paths.ensemble_dir}/{first_ens}/{first_mem}\"\n",
    "member_path = fs.glob(f\"{member_dir}/*.zarr\")[0]\n",
    "# member_data = xr.open_mfdataset('gs://' + member_path, engine='zarr').sel(time=slice(str(dates[0]), str(dates[-1])))\n",
    "member_data = xr.open_zarr('gs://' + member_path).sel(time=slice(str(dates[0]), str(dates[-1])))\n",
    "\n",
    "# Load reconstructed pCO₂ data\n",
    "recon_dir = f\"{saving_paths.recon_output_dir}/{first_ens}/{first_mem}\"    \n",
    "recon_path = f\"{recon_dir}/recon_pCO2_{first_ens}_{first_mem}_mon_1x1_{init_date}_{fin_date}.zarr\"\n",
    "full = xr.open_zarr(recon_path)[\"pCO2_recon_full\"]\n",
    "\n",
    "# Choose a specific month to visualize\n",
    "chosen_time = '2021-01'\n",
    "raw_data = member_data[\"spco2\"].sel(time=chosen_time).squeeze()\n",
    "recon_data = full.sel(time=chosen_time)[0, ...]\n",
    "\n",
    "# Shift longitudes from [0, 360] to [-180, 180] for global plotting\n",
    "raw_data = raw_data.roll(xlon=len(raw_data.xlon) // 2, roll_coords=True)\n",
    "recon_data = recon_data.roll(xlon=len(recon_data.xlon) // 2, roll_coords=True)\n",
    "\n",
    "# Load SOCAT mask and align longitude\n",
    "socat_mask_data = xr.open_zarr(saving_paths.socat_path).sel(time=slice(str(dates[0]),str(dates[-1])))\n",
    "mask = socat_mask_data.sel(time=chosen_time)[\"socat_mask\"].squeeze()\n",
    "mask = mask.roll(xlon=len(mask.xlon) // 2, roll_coords=True)\n",
    "\n",
    "# Mask original data where SOCAT mask == 0\n",
    "masked_raw = np.ma.masked_array(raw_data, mask=(mask == 0))\n",
    "\n",
    "# Start plotting side-by-side\n",
    "with plt.style.context(plot_style):\n",
    "#    fig = plt.figure(figsize=(10, 4), dpi=200)\n",
    "    fig = plt.figure(figsize=(8, 3), dpi=200)\n",
    "    worldmap = SpatialMap2(\n",
    "        fig=fig, region='world',\n",
    "        cbar_mode='single',  # Use one shared colorbar\n",
    "        colorbar=True,\n",
    "        cbar_location='bottom',\n",
    "        nrows_ncols=[1, 2]\n",
    "    )\n",
    "\n",
    "    # Plot original (masked) and reconstructed data\n",
    "    sub0 = worldmap.add_plot(\n",
    "        lon=raw_data['xlon'], lat=raw_data['ylat'], data=masked_raw,\n",
    "        vrange=vrange, cmap=cmap, ax=0\n",
    "    )\n",
    "    sub1 = worldmap.add_plot(\n",
    "        lon=recon_data['xlon'], lat=recon_data['ylat'], data=recon_data,\n",
    "        vrange=vrange, cmap=cmap, ax=1\n",
    "    )\n",
    "\n",
    "    worldmap.set_title(\"Original pCO₂ (2021-01)\", ax=0, fontsize=13)\n",
    "    worldmap.set_title(\"Reconstructed pCO₂ (2021-01)\", ax=1, fontsize=13)\n",
    "\n",
    "    colorbar = worldmap.add_colorbar(sub0, ax=0)\n",
    "    worldmap.set_cbar_xlabel(colorbar, cbar_title, fontsize=12)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8eede-3be3-4498-b6b3-19f5a2c9c65b",
   "metadata": {},
   "source": [
    "The figure compares the original sparse pCO₂ selected from the first ESM member, consistent with real-world sampling, and the corresponding machine-learning-based reconstruction for January 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7e41f-cb49-40a6-aec5-a8178353fd27",
   "metadata": {},
   "source": [
    "# 5. Evaluation of the reconstruction against the original model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238f61a-ef41-4df9-9ef9-ece5b0f7bcab",
   "metadata": {},
   "source": [
    "## 5.1 Create a combined dataset with reconstruction and original "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c5029-af92-4678-8ac6-1f716939e4ec",
   "metadata": {},
   "source": [
    "We concatenate the outputs and ground truth from all members and ESMs into a single dataset, and then use this combined dataset to calculate bias, RMSE, and correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb6e9d-fedc-4c5d-a87a-121972d9f233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for ensemble, members in selected_mems_dict.items():\n",
    "    mems_dict = {ensemble: members}  \n",
    "    ds = concat_datasets(mems_dict, recon_output_dir = saving_paths.recon_output_dir, init_date = init_date, fin_date=fin_date)\n",
    "    datasets.append(ds)\n",
    "concated_dataset = xr.concat(datasets, dim=\"ens\")\n",
    "evaluator = XarrayEvaluator(concated_dataset)\n",
    "\n",
    "ds_eval = evaluator.compute_all_metrics()\n",
    "print(ds_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec9749-619d-425b-97d8-15dbbd7fb205",
   "metadata": {},
   "source": [
    "## 5.2 Descriptive Statistics\n",
    "\n",
    "We can use ds_eval to easily compute descriptive statistics. In this example, we take the average across all time steps and ESMs:\n",
    "\n",
    "In this example, we select the Northern Hemisphere (ylat from 0 to 90), flatten the spatial dimensions, drop missing values, and generate a summary using describe(). This gives us a statistical overview (mean, std, min, max, etc.) of the bias in the Northern Hemisphere.\n",
    "\n",
    "You can change the selection for other regions and timeframes and ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e35d0b-58c2-4694-aabb-9c6b8301bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval_mean = (ds_eval['bias']*1).mean('ens').mean('time')\n",
    "ds_eval_mean.sel(ylat=slice(0,90)).stack(z=['ylat','xlon']).dropna('z').to_dataframe().describe()['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a172ba-f592-4e62-97c3-bbef03d536a2",
   "metadata": {},
   "source": [
    "## 5.3 Bias Visualizations\n",
    "Based on this, we could visualize bias between reconstruction and model truth, averaged over the 100 ensemble members, each with a \n",
    "monthly resolution over the period init_date through fin_date. Red and blue shading indicates regions where the reconstruction is biased high or low, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31f484-007d-4f11-8c14-8f24c1e419dc",
   "metadata": {},
   "source": [
    "### 5.3.1 How well does the reconstruction capture the mean pCO2 field? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2182c4d-be60-46cc-a132-c68b79bcc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_style = \"seaborn-v0_8-talk\"\n",
    "\n",
    "with plt.style.context(plot_style):\n",
    "    fig = plt.figure(figsize=(8.5, 11)) # fig = plt.figure(dpi=300)\n",
    "    worldmap = SpatialMap2(fig=fig, region='world', \n",
    "                           cbar_mode='single',  \n",
    "                           colorbar=True,  \n",
    "                           cbar_location='bottom',\n",
    "                           nrows_ncols=[1,1])\n",
    "    vrange = [-10, 10, 5] \n",
    "    cmap = cm.cm.balance\n",
    "    \n",
    "    data = (ds_eval['bias'] * 1).mean('ens').mean('member').mean('time')\n",
    "    data = data.roll(xlon=len(data.xlon) // 2, roll_coords=True)\n",
    "    \n",
    "    data = xr_add_cyclic_point(data, cyclic_coord='xlon') \n",
    "    sub = worldmap.add_plot(lon=data['xlon'], lat=data['ylat'], data=data, \n",
    "                            vrange=vrange[0:2], cmap=cm.cm.balance, ax=0, linewidth_coast=0.5)\n",
    "    \n",
    "    col = worldmap.add_colorbar(sub, ax=0, extend='both')\n",
    "    worldmap.set_cbar_xlabel(col, 'Mean bias [uatm]', fontsize=14)\n",
    "    worldmap.set_ticks(col, vrange[0], vrange[1], vrange[2])\n",
    "    col.ax.tick_params(labelsize=12)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77200d8f-27a6-404c-bbe6-7b1bc6d0df0d",
   "metadata": {},
   "source": [
    "### 5.3.2 Does ESM impact the estimate of the bias? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f3747-69fe-4733-9729-a430d7790da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_style = \"seaborn-v0_8-talk\"\n",
    "ensemble_means = ds_eval['bias'].mean(dim=['member', 'time'])\n",
    "\n",
    "ensemble_names = list(ds_eval['ens'].values) \n",
    "\n",
    "vrange = [-10,10, 5]\n",
    "cmap = cm.cm.balance\n",
    "\n",
    "num_ensemble = len(ensemble_names)  \n",
    "num_cols = 3  \n",
    "num_rows = (num_ensemble + num_cols - 1) // num_cols \n",
    "with plt.style.context(plot_style):\n",
    "    fig = plt.figure(dpi=300)\n",
    "    worldmap = SpatialMap2(fig=fig, region='world', \n",
    "                           cbar_mode='single',  \n",
    "                           colorbar=True,  \n",
    "                           cbar_location='bottom',\n",
    "                           nrows_ncols=[num_rows, num_cols]) \n",
    "    for i in range(num_ensemble):\n",
    "        data = ensemble_means.isel(ens=i)\n",
    "        data = data.roll(xlon=len(data.xlon) // 2, roll_coords=True)  \n",
    "        data = xr_add_cyclic_point(data, cyclic_coord='xlon') \n",
    "        sub = worldmap.add_plot(lon=data['xlon'], lat=data['ylat'], data=data, \n",
    "                                vrange=vrange[0:2], cmap=cmap, ax=i, linewidth_coast=0.5)\n",
    "        worldmap.set_title(title=ensemble_names[i], ax=i, fontsize=14)\n",
    "\n",
    "    col = worldmap.add_colorbar(sub, ax=0, extend='both')\n",
    "    worldmap.set_cbar_xlabel(col, 'Mean bias [uatm]', fontsize=14)\n",
    "\n",
    "    worldmap.set_ticks(col, tmin=vrange[0], tmax=vrange[1], dt=vrange[2])\n",
    "\n",
    "    col.ax.tick_params(labelsize=12)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35974847-357a-4aab-8a5e-1bb271b005ce",
   "metadata": {},
   "source": [
    "##  5.2 Reconstructed variability on seasonal, sub-decadal, and decadal timescales, compared to original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf3633-dde0-45e0-9e83-67b176c26543",
   "metadata": {},
   "source": [
    "Before computing spatial correlation metrics, we decompose both the reconstructed and reference pCO₂ fields into their long-term trend, seasonal cycle, and residual components using STL-like decomposition. We then evaluate their agreement by calculating gridwise correlation and standard deviation for each component across all ensemble members and ESMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a57a1-e28a-4fd7-bdb8-53150eeca665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The calculation of these statistics takes about 1 min/member\n",
    "ds_eval_corr = eval_spatial(selected_mems_dict, saving_paths.recon_output_dir, init_date, fin_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a175f-5738-46fa-b45d-0740bb76f52c",
   "metadata": {},
   "source": [
    "Phasing of reconstruction variability on seasonal, sub-decadal, and decadal, compared to original model. Correlation between \n",
    "reconstruction and original model on (a) seasonal, (b) sub-decadal, and (c) decadal time scales. \n",
    "\n",
    "Here, the average correlations across all ensemble members are shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838db47-8381-4557-85e5-78704fc16cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_style = \"seaborn-v0_8-talk\"\n",
    "\n",
    "with plt.style.context(plot_style):\n",
    "    # Setup Figure\n",
    "    fig = plt.figure(figsize=(8.5, 11)) # fig = plt.figure(dpi=300)\n",
    "    worldmap = SpatialMap2(fig=fig, region='world',\n",
    "                   cbar_mode='edge',\n",
    "                   axes_pad=0.15,\n",
    "                   colorbar=True,\n",
    "                   cbar_location='bottom',\n",
    "                   nrows_ncols=[3,1])\n",
    "    \n",
    "    # Colorbar ranges\n",
    "    vrange_col1 = [0, 1, 0.2]\n",
    "    cmap_col1 = cm.cm.oxy\n",
    "    \n",
    "    ##-----------------------------------------------------\n",
    "    ## Ensemble mean\n",
    "    ##-----------------------------------------------------\n",
    "    # Correlation - Annual-variation\n",
    "    data = ds_eval_corr['corr_seasonal'].mean('ens').mean('member')\n",
    "    data = data.roll(xlon=len(data.xlon) // 2, roll_coords=True)\n",
    "\n",
    "    data = xr_add_cyclic_point(data, cyclic_coord='xlon')\n",
    "    sub0 = worldmap.add_plot(lon=data['xlon'], lat=data['ylat'], data=data, \n",
    "                            vrange=vrange_col1[0:2], cmap=cmap_col1, ax=0)\n",
    "    \n",
    "    \n",
    "    # Correlation - sub-decadal\n",
    "    data = ds_eval_corr['corr_residual'].mean('ens').mean('member')\n",
    "    data = data.roll(xlon=len(data.xlon) // 2, roll_coords=True)\n",
    "    data = xr_add_cyclic_point(data, cyclic_coord='xlon')\n",
    "    sub2 = worldmap.add_plot(lon=data['xlon'], lat=data['ylat'], data=data, \n",
    "                            vrange=vrange_col1[0:2], cmap=cmap_col1, ax=1)\n",
    "    \n",
    "    # Correlation - decadal\n",
    "    data = ds_eval_corr['corr_dec'].mean('ens').mean('member')\n",
    "    data = data.roll(xlon=len(data.xlon) // 2, roll_coords=True)\n",
    "    data = xr_add_cyclic_point(data, cyclic_coord='xlon')\n",
    "    sub4 = worldmap.add_plot(lon=data['xlon'], lat=data['ylat'], data=data, \n",
    "                            vrange=vrange_col1[0:2], cmap=cmap_col1, ax=2)\n",
    "    \n",
    "    # add colorbar\n",
    "    col1 = worldmap.add_colorbar(sub0, ax=0, extend='min')\n",
    "    worldmap.set_cbar_xlabel(col1, f'Mean correlation')\n",
    "    worldmap.set_ticks(col1, vrange_col1[0], vrange_col1[1], vrange_col1[2])\n",
    "\n",
    "    worldmap.grid[0].text(-0.2, 0.5, \"Seasonal\", transform=worldmap.grid[0].transAxes,\n",
    "                           fontsize=14, fontweight='bold', va='center', ha='right', rotation=90)\n",
    "    \n",
    "    worldmap.grid[1].text(-0.2, 0.5, \"Sub-seasonal\", transform=worldmap.grid[1].transAxes,\n",
    "                           fontsize=14, fontweight='bold', va='center', ha='right', rotation=90)\n",
    "    \n",
    "    worldmap.grid[2].text(-0.2, 0.5, \"Decadal\", transform=worldmap.grid[2].transAxes,\n",
    "                           fontsize=14, fontweight='bold', va='center', ha='right', rotation=90)\n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71149bef-ea1c-4dec-a162-e2d824747b55",
   "metadata": {},
   "source": [
    "The reconstructed pCO₂ has highest fidelity on seasonal timescales, but is less accurate for sub-seasonal and decadal timescale variability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424c84e-ddb7-479b-af4b-2c514cbd9e11",
   "metadata": {},
   "source": [
    "### **Final Check for Redundant Files**  \n",
    "\n",
    "#### **For Both Owners and Reviewers:**  \n",
    "After execution is complete, review the project directory for any redundant files. Ensure that only necessary and relevant files are retained.  \n",
    "\n",
    "#### **For Reviewers:**  \n",
    "Once you have finished reviewing a project, you may delete files related to that project to free up storage space. However, be careful not to remove any data you still need.\n",
    "\n",
    "As an **owner**, your reconstruction data is stored under:  \n",
    "\n",
    "```\n",
    "gs://leap-persistent/{owner_username}/{owner_username}/pco2_residual/nmse/post02_xgb/reconstructions/\n",
    "```\n",
    "\n",
    "If you are **reviewing someone else’s project**, their data and experiment results will be stored under your username in the following path:  \n",
    "\n",
    "```\n",
    "gs://leap-persistent/{your_username}/{owner_username}/pco2_residual/nmse/post02_xgb/reconstructions/\n",
    "```\n",
    "\n",
    "After completing the review, you can delete the files under `{owner_username}` in your directory to free up storage space. This ensures that only necessary data is retained while removing redundant files from past reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb802b8b-779a-4a0e-9ad2-a47c20039dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runthiscell == -1:   # Only reviewers should delete data under this path. Everyone should clear redundant data, but be cautious not to delete necessary files.\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(\"leap-persistent\")\n",
    "    \n",
    "    prefix = f\"{your_username}/{owner_username}/pco2_residual/nmse/post02_xgb/reconstructions/\"\n",
    "    \n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    \n",
    "    files_deleted = 0\n",
    "    for blob in blobs:\n",
    "        try:\n",
    "            blob.delete()\n",
    "            print(f\"Deleted: {blob.name}\")\n",
    "            files_deleted += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete: {blob.name}\")\n",
    "            print(e)\n",
    "    \n",
    "    if files_deleted > 0:\n",
    "        print(f\"Successfully deleted {files_deleted} files under {reviewing_owner}'s directory.\")\n",
    "    else:\n",
    "        print(f\"No files found for {reviewing_owner}. Nothing was deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c1cf6-33d3-4e13-9bda-0b01d315d59d",
   "metadata": {},
   "source": [
    "# 6. Additional Information and Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f39a5-2aaf-45f4-ac09-b29471074a7c",
   "metadata": {},
   "source": [
    "This Python-based JupyterNotebook and associated utility files have been developed for Project 3 in the course EESC4243/STAT4243/5243 \"Climate Prediction Challenges with Machine Learning\", Professor Galen McKinley in DEES and Professor Tian Zheng in Statistics, Spring 2025 at Columbia University. The course is also a contribution from the NSF-supported LEAP STC and is intended to run on the LEAP-Pangeo cloud computing and data system. The Github repository for this course is at https://github.com/leap-stc/LEAPCourse-Climate-Pred-Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e14003-4686-4a17-aafb-bfdca4fcf680",
   "metadata": {},
   "source": [
    "Code developed by Course TA Xinyi Ke and Professor Galen McKinley, following from prior work from Dr. Thea Heimdal and Abby Shaum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13a819-43df-4d82-b963-e78910374e66",
   "metadata": {},
   "source": [
    "\n",
    "Additional references in which these methods are employed\\\n",
    "**Heimdal et al. (2024)** *\"Assessing improvements in global ocean pCO₂ machine learning reconstructions with Southern Ocean autonomous sampling.\"* **Biogeosciences** 21: 2159–2176.  \n",
    "([DOI: 10.5194/bg-21-2159-2024](https://doi.org/10.5194/bg-21-2159-2024))\\\n",
    "**Heimdal, T. H., & McKinley, G. A. (2024)** *\"The importance of adding unbiased Argo observations to the ocean carbon observing system.\"* **Scientific Reports**, 14(1), 19763. ([DOI: 10.1038/s41598-024-70617-x](https://doi.org/10.1038/s41598-024-70617-x) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
